{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization in Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Definition__\n",
    "\n",
    "In mathematics and computer science, an optimization problem is the problem of finding the best solution from all _feasible solutions_. \n",
    "\n",
    "__Standard form__\n",
    "\\begin{matrix} minimize_x & f(x) & \\\\ \n",
    "subject\\ to & g_i(x)\\ge 0,& i=1,...,m\\\\\n",
    "& h_j(x)=0, & j=1,...,p\n",
    "\\end{matrix}\n",
    "\n",
    "where\n",
    "- $f:\\mathbb{R}^n\\rightarrow \\mathbb{R}$ is the __object function__ to be minimized over the n-variable vector $x$,\n",
    "- $g_i(x)\\ge0$ are called __inequality constraints__,\n",
    "- $h_j(x)=0$ are called __equality constraints__, and\n",
    "- $m\\ge0$ and $p\\ge0$\n",
    "\n",
    "If $m$ and $p$ equal 0, the problem is an unconstrained optimization problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In machine learning, the object function is usually a loss(or cost) function which tells us “how good” our model\n",
    "and finds a solution that minimizes it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## __example)__ Linear regression\n",
    "\n",
    "- Figure1 : Regression\n",
    "![](./image/image1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Assume $f(x_i)=ax_i+b$. We have to find the $a$ and $b$\n",
    "\n",
    "In this example, the mean squar error(MSE) function is\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "L(a,b)\n",
    "&=\\frac{1}{N}\\sum_{i=1}^N(y_i-f(x_i))^2\\\\\n",
    "&=\\frac{1}{N}\\sum_{i=1}^N(y_i-(ax_i+b))^2\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "So, the optimization problem is\n",
    "\n",
    "\\begin{matrix} minimize_{a, b} & L(a,b) \n",
    "\\end{matrix}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Gradient descent method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The gradient descent method is one of the representative ways to apply the concept of differential to optimization problems and is to find the local minimum of a function. The gradient descent method is also called steepest descent method.\n",
    "\n",
    "- Figure2 : Gradient descent method\n",
    "![](./image/image2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a method of incrementally finding the parameter value to minimize the value of a certain loss function using the characteristics of the gradient.\n",
    "\n",
    "$$x_{k+1}=x_k-\\eta\\bigtriangledown f(x_k)$$\n",
    "\n",
    "where $\\eta\\ge0$ is the learning rate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "__Learning rate__\n",
    "\n",
    "The size of these steps is called the learning rate. With a high learning rate we can cover more ground each step, but we risk overshooting the lowest point since the slope of the hill is constantly changing. With a very low learning rate, we can confidently move in the direction of the negative gradient since we are recalculating it so frequently. A low learning rate is more precise, but calculating the gradient is time-consuming, so it will take us a very long time to get to the bottom."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying to the example above,\n",
    "\n",
    "\n",
    "$$a_{k+1}=a_k-\\eta\\frac{\\partial}{\\partial a} L(a_k,b_k)$$\n",
    "$$b_{k+1}=b_k-\\eta\\frac{\\partial}{\\partial b} L(a_k,b_k)$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Newton's method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Newton's method, also called Newton-Raphson method, is a useful method for approximating the solution of equation $f(x) = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "__Newton’s method for finding root of a function__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- Solve g(x)=0\n",
    "- Iteration method $x_{n+1}=x_n-\\frac{f(x_n)}{f'(x_n)}$\n",
    "- Figure3 :Newton's method\n",
    "![](./image/image3.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "__Newton method for finding minimum of a function__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- Since the derivative is 0 at local minimum, it is equivalent to solving $\\bigtriangledown f(x)=0$\n",
    "\n",
    "- Iteration method $x_{k+1}=x_k-\\mathit{H}(x_k)^{-1}\\bigtriangledown f(x_k)$\n",
    "    where hessian matrix $[\\mathit{H}]_{ij}=\\frac{\\partial^2f(x) }{\\partial x_i\\partial x_j}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The difference between the gradient descent method and the Newton method is that the gradient descent method requires a step size parameter λ, whereas the Newton method (Newton-Rapson method) or Gauss-Newton method does not need the step size parameter.\n",
    "\n",
    "This is because the Newton method automatically determines the step size from the current function and derivative (slope). \n",
    "\n",
    "Also, the Newton method or the Newton method has a fast convergence speed to find the solution and there is no problem that the convergence speed decreases near the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
